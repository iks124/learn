# CBOW with Negative Sampling (CBOW-NS)

本项目实现了基于 **CBOW (Continuous Bag of Words)** 和 **负采样 (Negative Sampling)** 的词向量训练模型，使用 PyTorch 框架完成。以下是模型结构、训练方式及相关细节的说明。

---

## 1. 模型结构

### CBOW-NS 模型
CBOW-NS 模型的核心结构如下：
- **输入嵌入层 (Input Embeddings)**：
  - 将上下文词索引映射为词向量。
  - 嵌入矩阵维度为 `(vocab_size, embed_size)`。
- **上下文向量平均 (Context Embedding)**：
  - 对上下文词向量取平均，生成上下文表示。
- **输出嵌入层 (Output Embeddings)**：
  - 将目标词索引映射为词向量。
- **负采样 (Negative Sampling)**：
  - 为每个正样本生成若干负样本，计算正负样本的得分。
- **损失函数**：
  - 使用 `log-sigmoid` 函数计算正样本得分的对数概率，并对负样本得分取负对数概率，最终取均值作为损失。

---

## 2. 数据预处理

### 输入数据
- 输入文件为分词后的语料文件，每个词之间用空格分隔。
- 示例文件路径：`data/zh.txt`。

### 数据处理流程
1. **读取语料**：
   - 将语料中的所有词读取为一个列表。
2. **构建词汇表**：
   - 统计词频，过滤低频词（频率低于 `MIN_COUNT` 的词将被丢弃）。
3. **生成训练样本**：
   - 对每个目标词，提取其上下文词作为输入。
   - 上下文窗口大小为 `[1, WINDOW_SIZE]` 的随机值。
4. **负采样分布**：
   - 根据词频的 $0.75$ 次幂生成负采样概率分布。

---

## 3. 训练方式

### 超参数
- **词向量维度 (Embedding Size)**：`200`
- **上下文窗口大小 (Window Size)**：`4`
- **负采样数量 (Negative Samples)**：`5`
- **批量大小 (Batch Size)**：`512`
- **学习率 (Learning Rate)**：`0.025`
- **训练轮数 (Epochs)**：`10`

### 训练流程
1. **数据加载**：
   - 使用 `DataLoader` 加载训练样本，支持批量训练。
2. **模型训练**：
   - 对每个批次：
     - 计算上下文词向量的平均值。
     - 计算目标词的正样本得分及负样本得分。
     - 使用负对数似然损失优化模型。
3. **保存词向量**：
   - 训练完成后，将输入嵌入层的词向量保存到文件中。

---

## 4. 文件说明

- **`cbow_ns.py`**：
  - 主程序，包含数据预处理、模型定义、训练循环及词向量保存。
- **`data/zh.txt`**：
  - 输入语料文件，需提前分词。
- **`code/zh_vectors.txt`**：
  - 输出的词向量文件，格式为：
    ```
    vocab_size embed_size
    word1 vec1_1 vec1_2 ... vec1_n
    word2 vec2_1 vec2_2 ... vec2_n
    ```
